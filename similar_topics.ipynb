{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f99e5a0c",
   "metadata": {},
   "source": [
    "# CATALYSTS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d681ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd8f0317",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Multi-Set Catalyst Topic Scan ===\n",
      "Search token: 'catalyst' (case-insensitive)\n",
      "Literature pattern: /home/siddharth/nas/chemquest_literature/*/metadata\n",
      "Patent pattern:     /home/siddharth/nas/chemquest_patents/*/metadata\n",
      "Results directory:  results/\n",
      "\n",
      "Found 17 literature metadata directories\n",
      "Found 9 patent metadata directories\n",
      "\n",
      "==================================================\n",
      "PROCESSING LITERATURE SETS\n",
      "==================================================\n",
      "  ✓ 605c72ef153207001f6470ce                         5 matches\n",
      "  ✓ 605c72ef153207001f6470d1                        16 matches\n",
      "  ✓ 605c72ef153207001f6470d2                        80 matches\n",
      "  ✓ 605c72ef153207001f6470d3                        67 matches\n",
      "  ✓ 605c72ef153207001f6470d4                       171 matches\n",
      "  ✓ 605c72ef153207001f6470d5                         0 matches\n",
      "  ✓ 605c72ef153207001f6470d6                         5 matches\n",
      "  ✓ 605c72ef153207001f6470d7                        59 matches\n",
      "  ✓ 605c72ef153207001f6470d8                        42 matches\n",
      "  ✓ 605c72ef153207001f6470d9                        32 matches\n",
      "  ✓ 605c72ef153207001f6470da                         6 matches\n",
      "  ✓ 605c72ef153207001f6470db                        28 matches\n",
      "  ✓ 605c72ef153207001f6470dc                         1 matches\n",
      "  ✓ 605c72ef153207001f6470dd                         1 matches\n",
      "  ✓ set_605c72ef153207001f6470cf                    46 matches\n",
      "  ✓ set_605c72ef153207001f6470d0                    10 matches\n",
      "  ✓ temporary_set_605c72ef153207001f6470ce           7 matches\n",
      "\n",
      "==================================================\n",
      "PROCESSING PATENT SETS (by year)\n",
      "==================================================\n",
      "  ✓ 2018                                             0 matches\n",
      "  ✓ 2019                                             0 matches\n",
      "  ✓ 2020                                             0 matches\n",
      "  ✓ 2021                                             0 matches\n",
      "  ✓ 2022                                           113 matches\n",
      "  ✓ 2023                                           114 matches\n",
      "  ✓ 2024                                           125 matches\n",
      "  ✓ 2025                                            89 matches\n",
      "  ✓ lubricant                                        2 matches\n",
      "\n",
      "==================================================\n",
      "SUMMARY\n",
      "==================================================\n",
      "Literature sets processed: 17\n",
      "Patent sets processed:     9\n",
      "Output files saved to:     results/\n",
      "\n",
      "Literature output files:\n",
      "  - 605c72ef153207001f6470ce_literature_hits.json         5 matches (    3.2 KB)\n",
      "  - 605c72ef153207001f6470d1_literature_hits.json        16 matches (    9.5 KB)\n",
      "  - 605c72ef153207001f6470d2_literature_hits.json        80 matches (   45.9 KB)\n",
      "  - 605c72ef153207001f6470d3_literature_hits.json        67 matches (   46.5 KB)\n",
      "  - 605c72ef153207001f6470d4_literature_hits.json       171 matches (  105.4 KB)\n",
      "  - 605c72ef153207001f6470d6_literature_hits.json         5 matches (    3.2 KB)\n",
      "  - 605c72ef153207001f6470d7_literature_hits.json        59 matches (   35.1 KB)\n",
      "  - 605c72ef153207001f6470d8_literature_hits.json        42 matches (   24.0 KB)\n",
      "  - 605c72ef153207001f6470d9_literature_hits.json        32 matches (   19.7 KB)\n",
      "  - 605c72ef153207001f6470da_literature_hits.json         6 matches (    3.8 KB)\n",
      "  - 605c72ef153207001f6470db_literature_hits.json        28 matches (   19.1 KB)\n",
      "  - 605c72ef153207001f6470dc_literature_hits.json         1 matches (    0.9 KB)\n",
      "  - 605c72ef153207001f6470dd_literature_hits.json         1 matches (    0.7 KB)\n",
      "  - set_605c72ef153207001f6470cf_literature_hits.json    46 matches (   32.4 KB)\n",
      "  - set_605c72ef153207001f6470d0_literature_hits.json    10 matches (    7.0 KB)\n",
      "  - temporary_set_605c72ef153207001f6470ce_literature_hits.json    7 matches (    4.6 KB)\n",
      "\n",
      "Patent output files:\n",
      "  - 2022_patent_hits.json                               113 matches (   51.2 KB)\n",
      "  - 2023_patent_hits.json                               114 matches (   51.5 KB)\n",
      "  - 2024_patent_hits.json                               125 matches (   55.0 KB)\n",
      "  - 2025_patent_hits.json                                89 matches (   40.6 KB)\n",
      "  - lubricant_patent_hits.json                            2 matches (    1.0 KB)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import ast\n",
    "import glob\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Dict, Any, Optional\n",
    "\n",
    "# ==========================\n",
    "# Configuration (edit these)\n",
    "# ==========================\n",
    "\n",
    "LITERATURE_PATTERN = \"/home/siddharth/nas/chemquest_literature/*/metadata\"\n",
    "PATENT_PATTERN = \"/home/siddharth/nas/chemquest_patents/*/metadata\"\n",
    "\n",
    "# Output directory\n",
    "RESULTS_DIR = \"results\"\n",
    "\n",
    "# Search token (case-insensitive)\n",
    "SEARCH_TOKEN = \"catalyst\"\n",
    "\n",
    "\n",
    "def safe_load_json(path: str) -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"Safely load a JSON file; return None if invalid or unreadable.\"\"\"\n",
    "    try:\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            return json.load(f)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "def normalize_list_or_string_field(value: Any) -> List[str]:\n",
    "    \"\"\"\n",
    "    Normalize a field that might be:\n",
    "    - a list of strings\n",
    "    - a single string\n",
    "    - a stringified Python list (e.g., \"['A', 'B']\")\n",
    "    Returns a list of strings (trimmed).\n",
    "    \"\"\"\n",
    "    if isinstance(value, list):\n",
    "        return [str(v).strip() for v in value]\n",
    "    \n",
    "    if isinstance(value, dict):\n",
    "        return []\n",
    "    \n",
    "    if isinstance(value, str):\n",
    "        s = value.strip()\n",
    "        parsed: Any = None\n",
    "        try:\n",
    "            parsed = ast.literal_eval(s)\n",
    "        except Exception:\n",
    "            parsed = None\n",
    "        \n",
    "        if isinstance(parsed, list):\n",
    "            return [str(v).strip() for v in parsed]\n",
    "        else:\n",
    "            return [s]\n",
    "    \n",
    "    return []\n",
    "\n",
    "\n",
    "def token_match_in_items(items: List[str], token: str) -> Tuple[bool, str, str]:\n",
    "    \"\"\"\n",
    "    Determine if token is present as exact item or substring.\n",
    "    Returns (matched, match_type, matched_text).\n",
    "    \"\"\"\n",
    "    t = token.lower()\n",
    "    for item in items:\n",
    "        text = item.strip()\n",
    "        low = text.lower()\n",
    "        \n",
    "        # exact item match (allow trivial pluralization)\n",
    "        if low == t or (low.endswith(\"s\") and low[:-1] == t):\n",
    "            return True, \"exact_item\", text\n",
    "        \n",
    "        # substring match\n",
    "        if t in low:\n",
    "            return True, \"substring\", text\n",
    "    \n",
    "    return False, \"\", \"\"\n",
    "\n",
    "\n",
    "def scan_literature_file(path: str, token: str) -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"Scan one literature JSON for token in 'categories'.\"\"\"\n",
    "    data = safe_load_json(path)\n",
    "    if not data:\n",
    "        return None\n",
    "    \n",
    "    categories_raw = data.get(\"categories\")\n",
    "    items = normalize_list_or_string_field(categories_raw)\n",
    "    matched, match_type, matched_text = token_match_in_items(items, token)\n",
    "    if matched:\n",
    "        return {\n",
    "            \"source_type\": \"literature\",\n",
    "            \"file\": path,\n",
    "            \"id\": data.get(\"chemrxiv_id\") or data.get(\"doi\") or os.path.basename(path),\n",
    "            \"title\": data.get(\"title\", \"\"),\n",
    "            \"match_type\": match_type,\n",
    "            \"matched_text\": matched_text,\n",
    "            \"all_categories\": items,\n",
    "        }\n",
    "    return None\n",
    "\n",
    "\n",
    "def scan_patent_file(path: str, token: str) -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"Scan one patent JSON for token in 'classifications'.\"\"\"\n",
    "    data = safe_load_json(path)\n",
    "    if not data:\n",
    "        return None\n",
    "    \n",
    "    classifications_raw = data.get(\"classifications\") or data.get(\"classification\")\n",
    "    items = normalize_list_or_string_field(classifications_raw)\n",
    "    matched, match_type, matched_text = token_match_in_items(items, token)\n",
    "    if matched:\n",
    "        return {\n",
    "            \"source_type\": \"patent\",\n",
    "            \"file\": path,\n",
    "            \"id\": data.get(\"patent\") or data.get(\"application_number\") or data.get(\"title\") or os.path.basename(path),\n",
    "            \"title\": data.get(\"title\", \"\"),\n",
    "            \"match_type\": match_type,\n",
    "            \"matched_text\": matched_text,\n",
    "            \"all_classifications\": items,\n",
    "        }\n",
    "    return None\n",
    "\n",
    "\n",
    "def iter_json_files(root_dir: str):\n",
    "    \"\"\"Yield full paths of .json files under root_dir recursively.\"\"\"\n",
    "    for r, _, files in os.walk(root_dir):\n",
    "        for fn in files:\n",
    "            if fn.lower().endswith(\".json\"):\n",
    "                yield os.path.join(r, fn)\n",
    "\n",
    "\n",
    "def extract_set_name(metadata_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract the set name from a metadata directory path.\n",
    "    E.g., /home/.../chemquest_literature/set1/metadata -> set1\n",
    "    \"\"\"\n",
    "    path = Path(metadata_path)\n",
    "    # Parent of metadata is the set directory\n",
    "    return path.parent.name\n",
    "\n",
    "\n",
    "def find_metadata_directories(pattern: str) -> List[Tuple[str, str]]:\n",
    "    \"\"\"\n",
    "    Find all metadata directories matching the pattern.\n",
    "    Returns list of (set_name, full_metadata_path) tuples.\n",
    "    \"\"\"\n",
    "    matches = glob.glob(pattern)\n",
    "    result = []\n",
    "    for match in matches:\n",
    "        if os.path.isdir(match):\n",
    "            set_name = extract_set_name(match)\n",
    "            result.append((set_name, match))\n",
    "    return result\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Create results directory\n",
    "    os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "    \n",
    "    # Find all metadata directories\n",
    "    lit_dirs = find_metadata_directories(LITERATURE_PATTERN)\n",
    "    pat_dirs = find_metadata_directories(PATENT_PATTERN)\n",
    "    \n",
    "    print(\"=== Multi-Set Catalyst Topic Scan ===\")\n",
    "    print(f\"Search token: '{SEARCH_TOKEN}' (case-insensitive)\")\n",
    "    print(f\"Literature pattern: {LITERATURE_PATTERN}\")\n",
    "    print(f\"Patent pattern:     {PATENT_PATTERN}\")\n",
    "    print(f\"Results directory:  {RESULTS_DIR}/\")\n",
    "    print()\n",
    "    \n",
    "    print(f\"Found {len(lit_dirs)} literature metadata directories\")\n",
    "    print(f\"Found {len(pat_dirs)} patent metadata directories\")\n",
    "    print()\n",
    "    \n",
    "    # ==========================================\n",
    "    # Process LITERATURE sets (by set ID)\n",
    "    # ==========================================\n",
    "    print(\"=\" * 50)\n",
    "    print(\"PROCESSING LITERATURE SETS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for set_name, metadata_dir in sorted(lit_dirs):\n",
    "        literature_hits: List[Dict[str, Any]] = []\n",
    "        \n",
    "        for json_path in iter_json_files(metadata_dir):\n",
    "            hit = scan_literature_file(json_path, SEARCH_TOKEN)\n",
    "            if hit:\n",
    "                literature_hits.append(hit)\n",
    "        \n",
    "        # Save literature results\n",
    "        lit_output = os.path.join(RESULTS_DIR, f\"{set_name}_literature_hits.json\")\n",
    "        with open(lit_output, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump({\n",
    "                \"set_name\": set_name,\n",
    "                \"search_token\": SEARCH_TOKEN,\n",
    "                \"total_matches\": len(literature_hits),\n",
    "                \"matches\": literature_hits\n",
    "            }, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        print(f\"  ✓ {set_name:<45} {len(literature_hits):>4} matches\")\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    # ==========================================\n",
    "    # Process PATENT sets (by year)\n",
    "    # ==========================================\n",
    "    print(\"=\" * 50)\n",
    "    print(\"PROCESSING PATENT SETS (by year)\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for set_name, metadata_dir in sorted(pat_dirs):\n",
    "        patent_hits: List[Dict[str, Any]] = []\n",
    "        \n",
    "        for json_path in iter_json_files(metadata_dir):\n",
    "            hit = scan_patent_file(json_path, SEARCH_TOKEN)\n",
    "            if hit:\n",
    "                patent_hits.append(hit)\n",
    "        \n",
    "        # Save patent results\n",
    "        pat_output = os.path.join(RESULTS_DIR, f\"{set_name}_patent_hits.json\")\n",
    "        with open(pat_output, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump({\n",
    "                \"set_name\": set_name,\n",
    "                \"search_token\": SEARCH_TOKEN,\n",
    "                \"total_matches\": len(patent_hits),\n",
    "                \"matches\": patent_hits\n",
    "            }, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        print(f\"  ✓ {set_name:<45} {len(patent_hits):>4} matches\")\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    # ==========================================\n",
    "    # Summary\n",
    "    # ==========================================\n",
    "    print(\"=\" * 50)\n",
    "    print(\"SUMMARY\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Literature sets processed: {len(lit_dirs)}\")\n",
    "    print(f\"Patent sets processed:     {len(pat_dirs)}\")\n",
    "    print(f\"Output files saved to:     {RESULTS_DIR}/\")\n",
    "    print()\n",
    "    \n",
    "    # List all output files organized by type\n",
    "    lit_files = sorted(glob.glob(os.path.join(RESULTS_DIR, \"*_literature_hits.json\")))\n",
    "    pat_files = sorted(glob.glob(os.path.join(RESULTS_DIR, \"*_patent_hits.json\")))\n",
    "    \n",
    "    print(\"Literature output files:\")\n",
    "    for f in lit_files:\n",
    "        with open(f, 'r') as fh:\n",
    "            data = json.load(fh)\n",
    "            count = data.get('total_matches', 0)\n",
    "        if count > 0:  # Only show non-empty files\n",
    "            size_kb = os.path.getsize(f) / 1024\n",
    "            print(f\"  - {os.path.basename(f):<50} {count:>4} matches ({size_kb:>7.1f} KB)\")\n",
    "    \n",
    "    print()\n",
    "    print(\"Patent output files:\")\n",
    "    for f in pat_files:\n",
    "        with open(f, 'r') as fh:\n",
    "            data = json.load(fh)\n",
    "            count = data.get('total_matches', 0)\n",
    "        if count > 0:  # Only show non-empty files\n",
    "            size_kb = os.path.getsize(f) / 1024\n",
    "            print(f\"  - {os.path.basename(f):<50} {count:>4} matches ({size_kb:>7.1f} KB)\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb4b537",
   "metadata": {},
   "source": [
    "### moving to temporary directory inside nas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a447ac9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Building multi_collection_dataset for topic: catalysts\n",
      "Literature parent: /home/siddharth/nas/chemquest_literature\n",
      "Patent parent:     /home/siddharth/nas/chemquest_patents\n",
      "Results dir:       results\n",
      "Destination base:  /home/siddharth/nas/multi_collection_dataset\n",
      "\n",
      "Processing literature hits...\n",
      "Processing patent hits...\n",
      "Done.\n",
      "Destination structure:\n",
      "  - /home/siddharth/nas/multi_collection_dataset/catalysts/literatures\n",
      "  - /home/siddharth/nas/multi_collection_dataset/catalysts/patents\n",
      "Failed IDs logs (if any):\n",
      "  - /home/siddharth/nas/multi_collection_dataset/catalysts/failed_literature_ids.txt\n",
      "  - /home/siddharth/nas/multi_collection_dataset/catalysts/failed_patent_ids.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, List, Tuple\n",
    "\n",
    "# ==========================\n",
    "# CONFIGURATION\n",
    "# ==========================\n",
    "\n",
    "# Parent dirs where original chemquest data lives\n",
    "LITERATURE_PARENT = \"/home/siddharth/nas/chemquest_literature\"\n",
    "PATENT_PARENT = \"/home/siddharth/nas/chemquest_patents\"\n",
    "\n",
    "# Where the scan results JSONs ( *_literature_hits.json / *_patent_hits.json ) live\n",
    "RESULTS_DIR = \"results\"\n",
    "\n",
    "# Destination base dir for this topic\n",
    "TOPIC_NAME = \"catalysts\"  # used in destination path only, not for matching\n",
    "\n",
    "DEST_BASE = \"/home/siddharth/nas/multi_collection_dataset\"\n",
    "\n",
    "# Derived destination dirs\n",
    "DEST_LIT_BASE = os.path.join(DEST_BASE, TOPIC_NAME, \"literatures\")\n",
    "DEST_PAT_BASE = os.path.join(DEST_BASE, TOPIC_NAME, \"patents\")\n",
    "\n",
    "# Subdirs under each of the above\n",
    "SUBDIRS = [\"metadata\", \"markdown\", \"json\"]\n",
    "\n",
    "# Failed IDs logs\n",
    "FAILED_LIT_IDS_FILE = os.path.join(DEST_BASE, TOPIC_NAME, \"failed_literature_ids.txt\")\n",
    "FAILED_PAT_IDS_FILE = os.path.join(DEST_BASE, TOPIC_NAME, \"failed_patent_ids.txt\")\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# HELPERS\n",
    "# ==========================\n",
    "\n",
    "def ensure_dest_dirs():\n",
    "    \"\"\"Create destination directories (read-only with respect to source).\"\"\"\n",
    "    for base in [DEST_LIT_BASE, DEST_PAT_BASE]:\n",
    "        for sub in SUBDIRS:\n",
    "            os.makedirs(os.path.join(base, sub), exist_ok=True)\n",
    "    # Also ensure directory for failed logs\n",
    "    os.makedirs(os.path.dirname(FAILED_LIT_IDS_FILE), exist_ok=True)\n",
    "\n",
    "\n",
    "def load_hits(path: str) -> Dict[str, Any]:\n",
    "    \"\"\"Load one *_hits.json file.\"\"\"\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "\n",
    "def iter_result_files(pattern_suffix: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    List all result files with given suffix inside RESULTS_DIR.\n",
    "    E.g., pattern_suffix = '_literature_hits.json' or '_patent_hits.json'.\n",
    "    \"\"\"\n",
    "    files = []\n",
    "    for fn in os.listdir(RESULTS_DIR):\n",
    "        if fn.endswith(pattern_suffix):\n",
    "            files.append(os.path.join(RESULTS_DIR, fn))\n",
    "    return sorted(files)\n",
    "\n",
    "\n",
    "def resolve_source_paths_for_literature(src_meta_path: str) -> Tuple[Path, Path, Path]:\n",
    "    \"\"\"\n",
    "    Given 'file' path from literature hit (metadata path),\n",
    "    resolve the 3 source files: metadata, markdown, json.\n",
    "    Assumes structure:\n",
    "        /.../chemquest_literature/<set_id>/metadata/<id>.json\n",
    "        /.../chemquest_literature/<set_id>/markdown/<id>.md (or .markdown)\n",
    "        /.../chemquest_literature/<set_id>/json/<id>.json\n",
    "    \"\"\"\n",
    "    meta = Path(src_meta_path)\n",
    "    set_dir = meta.parent.parent  # .../<set_id>/\n",
    "    stem = meta.stem\n",
    "\n",
    "    meta_path = set_dir / \"metadata\" / f\"{stem}.json\"\n",
    "    # markdown could be .md; adjust if your real extension differs\n",
    "    md_path = set_dir / \"markdown\" / f\"{stem}.md\"\n",
    "    json_path = set_dir / \"json\" / f\"{stem}.json\"\n",
    "\n",
    "    return meta_path, md_path, json_path\n",
    "\n",
    "\n",
    "def resolve_source_paths_for_patent(src_meta_path: str) -> Tuple[Path, Path, Path]:\n",
    "    \"\"\"\n",
    "    Given 'file' path from patent hit (metadata path),\n",
    "    resolve the 3 source files: metadata, markdown, json.\n",
    "    Assumes structure:\n",
    "        /.../chemquest_patents/<year_or_set>/metadata/<id>.json\n",
    "        /.../chemquest_patents/<year_or_set>/markdown/<id>.md\n",
    "        /.../chemquest_patents/<year_or_set>/json/<id>.json\n",
    "    \"\"\"\n",
    "    meta = Path(src_meta_path)\n",
    "    set_dir = meta.parent.parent  # .../<year_or_set>/\n",
    "    stem = meta.stem\n",
    "\n",
    "    meta_path = set_dir / \"metadata\" / f\"{stem}.json\"\n",
    "    md_path = set_dir / \"markdown\" / f\"{stem}.md\"\n",
    "    json_path = set_dir / \"json\" / f\"{stem}.json\"\n",
    "\n",
    "    return meta_path, md_path, json_path\n",
    "\n",
    "\n",
    "def safe_copy(src: Path, dst: Path) -> bool:\n",
    "    \"\"\"\n",
    "    Copy file from src to dst if src exists.\n",
    "    Returns True if copied, False if src missing.\n",
    "    Never modifies or deletes src.\n",
    "    \"\"\"\n",
    "    if not src.is_file():\n",
    "        return False\n",
    "    os.makedirs(dst.parent, exist_ok=True)\n",
    "    # copy2 preserves metadata; source unchanged\n",
    "    shutil.copy2(str(src), str(dst))\n",
    "    return True\n",
    "\n",
    "\n",
    "def process_literature_hits():\n",
    "    failed_ids: List[str] = []\n",
    "    files = iter_result_files(\"_literature_hits.json\")\n",
    "\n",
    "    for hits_file in files:\n",
    "        data = load_hits(hits_file)\n",
    "        matches = data.get(\"matches\", [])\n",
    "        for m in matches:\n",
    "            src_meta_path = m.get(\"file\")\n",
    "            if not src_meta_path:\n",
    "                continue\n",
    "\n",
    "            # Resolve source paths under chemquest_literature\n",
    "            meta_path, md_path, json_path = resolve_source_paths_for_literature(src_meta_path)\n",
    "            basename = meta_path.stem\n",
    "\n",
    "            # Destination paths\n",
    "            dst_meta = Path(DEST_LIT_BASE) / \"metadata\" / f\"{basename}.json\"\n",
    "            dst_md = Path(DEST_LIT_BASE) / \"markdown\" / f\"{basename}.md\"\n",
    "            dst_json = Path(DEST_LIT_BASE) / \"json\" / f\"{basename}.json\"\n",
    "\n",
    "            ok_meta = safe_copy(meta_path, dst_meta)\n",
    "            ok_md = safe_copy(md_path, dst_md)\n",
    "            ok_json = safe_copy(json_path, dst_json)\n",
    "\n",
    "            if not (ok_meta and ok_md and ok_json):\n",
    "                failed_ids.append(basename)\n",
    "\n",
    "    # Write failed IDs log\n",
    "    if failed_ids:\n",
    "        with open(FAILED_LIT_IDS_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "            for _id in sorted(set(failed_ids)):\n",
    "                f.write(_id + \"\\n\")\n",
    "\n",
    "\n",
    "def process_patent_hits():\n",
    "    failed_ids: List[str] = []\n",
    "    files = iter_result_files(\"_patent_hits.json\")\n",
    "\n",
    "    for hits_file in files:\n",
    "        data = load_hits(hits_file)\n",
    "        matches = data.get(\"matches\", [])\n",
    "        for m in matches:\n",
    "            src_meta_path = m.get(\"file\")\n",
    "            if not src_meta_path:\n",
    "                continue\n",
    "\n",
    "            # Resolve source paths under chemquest_patents\n",
    "            meta_path, md_path, json_path = resolve_source_paths_for_patent(src_meta_path)\n",
    "            basename = meta_path.stem\n",
    "\n",
    "            # Destination paths\n",
    "            dst_meta = Path(DEST_PAT_BASE) / \"metadata\" / f\"{basename}.json\"\n",
    "            dst_md = Path(DEST_PAT_BASE) / \"markdown\" / f\"{basename}.md\"\n",
    "            dst_json = Path(DEST_PAT_BASE) / \"json\" / f\"{basename}.json\"\n",
    "\n",
    "            ok_meta = safe_copy(meta_path, dst_meta)\n",
    "            ok_md = safe_copy(md_path, dst_md)\n",
    "            ok_json = safe_copy(json_path, dst_json)\n",
    "\n",
    "            if not (ok_meta and ok_md and ok_json):\n",
    "                failed_ids.append(basename)\n",
    "\n",
    "    # Write failed IDs log\n",
    "    if failed_ids:\n",
    "        with open(FAILED_PAT_IDS_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "            for _id in sorted(set(failed_ids)):\n",
    "                f.write(_id + \"\\n\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    print(\"=== Building multi_collection_dataset for topic:\", TOPIC_NAME)\n",
    "    print(\"Literature parent:\", LITERATURE_PARENT)\n",
    "    print(\"Patent parent:    \", PATENT_PARENT)\n",
    "    print(\"Results dir:      \", RESULTS_DIR)\n",
    "    print(\"Destination base: \", DEST_BASE)\n",
    "    print()\n",
    "\n",
    "    ensure_dest_dirs()\n",
    "\n",
    "    print(\"Processing literature hits...\")\n",
    "    process_literature_hits()\n",
    "    print(\"Processing patent hits...\")\n",
    "    process_patent_hits()\n",
    "\n",
    "    print(\"Done.\")\n",
    "    print(\"Destination structure:\")\n",
    "    print(\"  -\", DEST_LIT_BASE)\n",
    "    print(\"  -\", DEST_PAT_BASE)\n",
    "    print(\"Failed IDs logs (if any):\")\n",
    "    print(\"  -\", FAILED_LIT_IDS_FILE)\n",
    "    print(\"  -\", FAILED_PAT_IDS_FILE)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f85d931",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "docling",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
