INFO 11-28 09:51:25 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=2048.
[1;36m(APIServer pid=56400)[0;0m INFO 11-28 09:51:25 [api_server.py:1977] vLLM API server version 0.11.2
[1;36m(APIServer pid=56400)[0;0m INFO 11-28 09:51:25 [utils.py:253] non-default args: {'model_tag': 'datalab-to/chandra', 'host': '0.0.0.0', 'model': 'datalab-to/chandra', 'trust_remote_code': True, 'gpu_memory_utilization': 0.8}
[1;36m(APIServer pid=56400)[0;0m INFO 11-28 09:51:26 [model.py:631] Resolved architecture: Qwen3VLForConditionalGeneration
[1;36m(APIServer pid=56400)[0;0m INFO 11-28 09:51:26 [model.py:1745] Using max model len 262144
[1;36m(APIServer pid=56400)[0;0m INFO 11-28 09:51:26 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=2048.
[1;36m(EngineCore_DP0 pid=56519)[0;0m INFO 11-28 09:51:31 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='datalab-to/chandra', speculative_config=None, tokenizer='datalab-to/chandra', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=262144, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=datalab-to/chandra, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=56519)[0;0m INFO 11-28 09:51:32 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://192.168.1.6:39845 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=56519)[0;0m INFO 11-28 09:51:40 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=56519)[0;0m INFO 11-28 09:51:47 [gpu_model_runner.py:3259] Starting to load model datalab-to/chandra...
[1;36m(EngineCore_DP0 pid=56519)[0;0m INFO 11-28 09:51:48 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=56519)[0;0m INFO 11-28 09:51:48 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=56519)[0;0m INFO 11-28 09:51:51 [default_loader.py:314] Loading weights took 1.73 seconds
[1;36m(EngineCore_DP0 pid=56519)[0;0m INFO 11-28 09:51:51 [gpu_model_runner.py:3338] Model loading took 16.6397 GiB memory and 3.232343 seconds
[1;36m(EngineCore_DP0 pid=56519)[0;0m INFO 11-28 09:51:51 [gpu_model_runner.py:4088] Encoder cache will be initialized with a budget of 153600 tokens, and profiled with 1 video items of the maximum feature size.
[1;36m(EngineCore_DP0 pid=56519)[0;0m INFO 11-28 09:52:02 [backends.py:631] Using cache directory: /home/siddharth/.cache/vllm/torch_compile_cache/15217e2f1d/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=56519)[0;0m INFO 11-28 09:52:02 [backends.py:647] Dynamo bytecode transform time: 3.65 s
[1;36m(EngineCore_DP0 pid=56519)[0;0m INFO 11-28 09:52:06 [backends.py:210] Directly load the compiled graph(s) for dynamic shape from the cache, took 3.873 s
[1;36m(EngineCore_DP0 pid=56519)[0;0m INFO 11-28 09:52:06 [monitor.py:34] torch.compile takes 7.53 s in total
[1;36m(EngineCore_DP0 pid=56519)[0;0m ERROR 11-28 09:52:07 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=56519)[0;0m ERROR 11-28 09:52:07 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=56519)[0;0m ERROR 11-28 09:52:07 [core.py:842]   File "/home/siddharth/miniconda3/envs/chandra/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3938, in _dummy_sampler_run
[1;36m(EngineCore_DP0 pid=56519)[0;0m ERROR 11-28 09:52:07 [core.py:842]     sampler_output = self.sampler(
[1;36m(EngineCore_DP0 pid=56519)[0;0m ERROR 11-28 09:52:07 [core.py:842]                      ^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=56519)[0;0m ERROR 11-28 09:52:07 [core.py:842]   File "/home/siddharth/miniconda3/envs/chandra/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=56519)[0;0m ERROR 11-28 09:52:07 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=56519)[0;0m ERROR 11-28 09:52:07 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=56519)[0;0m ERROR 11-28 09:52:07 [core.py:842]   File "/home/siddharth/miniconda3/envs/chandra/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=56519)[0;0m ERROR 11-28 09:52:07 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=56519)[0;0m ERROR 11-28 09:52:07 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=56519)[0;0m ERROR 11-28 09:52:07 [core.py:842]   File "/home/siddharth/miniconda3/envs/chandra/lib/python3.12/site-packages/vllm/v1/sample/sampler.py", line 93, in forward
[1;36m(EngineCore_DP0 pid=56519)[0;0m ERROR 11-28 09:52:07 [core.py:842]     sampled, processed_logprobs = self.sample(logits, sampling_metadata)
[1;36m(EngineCore_DP0 pid=56519)[0;0m ERROR 11-28 09:52:07 [core.py:842]                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=56519)[0;0m ERROR 11-28 09:52:07 [core.py:842]   File "/home/siddharth/miniconda3/envs/chandra/lib/python3.12/site-packages/vllm/v1/sample/sampler.py", line 184, in sample
[1;36m(EngineCore_DP0 pid=56519)[0;0m ERROR 11-28 09:52:07 [core.py:842]     random_sampled, processed_logprobs = self.topk_topp_sampler(
[1;36m(EngineCore_DP0 pid=56519)[0;0m ERROR 11-28 09:52:07 [core.py:842]                                          ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=56519)[0;0m ERROR 11-28 09:52:07 [core.py:842]   File "/home/siddharth/miniconda3/envs/chandra/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=56519)[0;0m ERROR 11-28 09:52:07 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=56519)[0;0m ERROR 11-28 09:52:07 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=56519)[0;0m ERROR 11-28 09:52:07 [core.py:842]   File "/home/siddharth/miniconda3/envs/chandra/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=56519)[0;0m ERROR 11-28 09:52:07 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=56519)[0;0m ERROR 11-28 09:52:07 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=56519)[0;0m ERROR 11-28 09:52:07 [core.py:842]   File "/home/siddharth/miniconda3/envs/chandra/lib/python3.12/site-packages/vllm/v1/sample/ops/topk_topp_sampler.py", line 75, in forward_native
[1;36m(EngineCore_DP0 pid=56519)[0;0m ERROR 11-28 09:52:07 [core.py:842]     logits = self.apply_top_k_top_p(logits, k, p)
[1;36m(EngineCore_DP0 pid=56519)[0;0m ERROR 11-28 09:52:07 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=56519)[0;0m ERROR 11-28 09:52:07 [core.py:842]   File "/home/siddharth/miniconda3/envs/chandra/lib/python3.12/site-packages/vllm/v1/sample/ops/topk_topp_sampler.py", line 171, in apply_top_k_top_p
[1;36m(EngineCore_DP0 pid=56519)[0;0m ERROR 11-28 09:52:07 [core.py:842]     logits_sort, logits_idx = logits.sort(dim=-1, descending=False)
[1;36m(EngineCore_DP0 pid=56519)[0;0m ERROR 11-28 09:52:07 [core.py:842]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=56519)[0;0m ERROR 11-28 09:52:07 [core.py:842] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 446.00 MiB. GPU 0 has a total capacity of 23.51 GiB of which 35.69 MiB is free. Including non-PyTorch memory, this process has 23.38 GiB memory in use. Of the allocated memory 22.84 GiB is allocated by PyTorch, and 67.31 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(EngineCore_DP0 pid=56519)[0;0m ERROR 11-28 09:52:07 [core.py:842] 
[1;36m(EngineCore_DP0 pid=56519)[0;0m ERROR 11-28 09:52:07 [core.py:842] The above exception was the direct cause of the following exception:
[1;36m(EngineCore_DP0 pid=56519)[0;0m ERROR 11-28 09:52:07 [core.py:842] 
[1;36m(EngineCore_DP0 pid=56519)[0;0m ERROR 11-28 09:52:07 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=56519)[0;0m ERROR 11-28 09:52:07 [core.py:842]   File "/home/siddharth/miniconda3/envs/chandra/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=56519)[0;0m ERROR 11-28 09:52:07 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=56519)[0;0m ERROR 11-28 09:52:07 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=56519)[0;0m ERROR 11-28 09:52:07 [core.py:842]   File "/home/siddharth/miniconda3/envs/chandra/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=56519)[0;0m ERROR 11-28 09:52:07 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=56519)[0;0m ERROR 11-28 09:52:07 [core.py:842]   File "/home/siddharth/miniconda3/envs/chandra/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=56519)[0;0m ERROR 11-28 09:52:07 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=56519)[0;0m ERROR 11-28 09:52:07 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=56519)[0;0m ERROR 11-28 09:52:07 [core.py:842]   File "/home/siddharth/miniconda3/envs/chandra/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=56519)[0;0m ERROR 11-28 09:52:07 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=56519)[0;0m ERROR 11-28 09:52:07 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=56519)[0;0m ERROR 11-28 09:52:07 [core.py:842]   File "/home/siddharth/miniconda3/envs/chandra/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=56519)[0;0m ERROR 11-28 09:52:07 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=56519)[0;0m ERROR 11-28 09:52:07 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=56519)[0;0m ERROR 11-28 09:52:07 [core.py:842]   File "/home/siddharth/miniconda3/envs/chandra/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=56519)[0;0m ERROR 11-28 09:52:07 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=56519)[0;0m ERROR 11-28 09:52:07 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=56519)[0;0m ERROR 11-28 09:52:07 [core.py:842]   File "/home/siddharth/miniconda3/envs/chandra/lib/python3.12/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=56519)[0;0m ERROR 11-28 09:52:07 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=56519)[0;0m ERROR 11-28 09:52:07 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=56519)[0;0m ERROR 11-28 09:52:07 [core.py:842]   File "/home/siddharth/miniconda3/envs/chandra/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=56519)[0;0m ERROR 11-28 09:52:07 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=56519)[0;0m ERROR 11-28 09:52:07 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=56519)[0;0m ERROR 11-28 09:52:07 [core.py:842]   File "/home/siddharth/miniconda3/envs/chandra/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=56519)[0;0m ERROR 11-28 09:52:07 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=56519)[0;0m ERROR 11-28 09:52:07 [core.py:842]   File "/home/siddharth/miniconda3/envs/chandra/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4142, in profile_run
[1;36m(EngineCore_DP0 pid=56519)[0;0m ERROR 11-28 09:52:07 [core.py:842]     output = self._dummy_sampler_run(last_hidden_states)
[1;36m(EngineCore_DP0 pid=56519)[0;0m ERROR 11-28 09:52:07 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=56519)[0;0m ERROR 11-28 09:52:07 [core.py:842]   File "/home/siddharth/miniconda3/envs/chandra/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=56519)[0;0m ERROR 11-28 09:52:07 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=56519)[0;0m ERROR 11-28 09:52:07 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=56519)[0;0m ERROR 11-28 09:52:07 [core.py:842]   File "/home/siddharth/miniconda3/envs/chandra/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3943, in _dummy_sampler_run
[1;36m(EngineCore_DP0 pid=56519)[0;0m ERROR 11-28 09:52:07 [core.py:842]     raise RuntimeError(
[1;36m(EngineCore_DP0 pid=56519)[0;0m ERROR 11-28 09:52:07 [core.py:842] RuntimeError: CUDA out of memory occurred when warming up sampler with 256 dummy requests. Please try lowering `max_num_seqs` or `gpu_memory_utilization` when initializing the engine.
